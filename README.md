# Gemini

**Elixir client for Google's Gemini API**

A comprehensive, production-ready Elixir library for interacting with Google's Gemini generative AI API. Supports text generation, multimodal content, streaming, chat sessions, and more.

## Features

- ✅ **Text Generation** - Generate text content with configurable parameters
- ✅ **Multimodal Support** - Work with text, images, and other media types
- ✅ **Real-Time Streaming** - True streaming with progressive text generation (not batch responses)
- ✅ **Chat Sessions** - Multi-turn conversations with context preservation
- ✅ **Model Management** - List and query available Gemini models
- ✅ **Token Counting** - Calculate token usage for cost estimation
- ✅ **Safety Controls** - Configure content safety settings
- ✅ **Error Handling** - Structured error types with detailed information
- ✅ **Type Safety** - Full TypeSpec coverage with typed structs
- ✅ **Configuration** - Flexible configuration via environment variables or application config

## Installation

Add `gemini` to your list of dependencies in `mix.exs`:

```elixir
def deps do
  [
    {:gemini, "~> 0.1.0"}
  ]
end
```

## Quick Start

### 1. Get an API Key

Get your Gemini API key from [Google AI Studio](https://aistudio.google.com/app/apikey).

### 2. Configure

Set your API key either as an environment variable:

```bash
export GEMINI_API_KEY="your_api_key_here"
```

Or in your application configuration:

```elixir
config :gemini, api_key: "your_api_key_here"
```

### 3. Generate Content

```elixir
# Simple text generation
{:ok, text} = Gemini.text("What is the capital of France?")
IO.puts(text)
# => "The capital of France is Paris."

# More detailed response
{:ok, response} = Gemini.generate("Explain quantum physics")
{:ok, text} = Gemini.extract_text(response)
```

## Usage Examples

### Basic Text Generation

```elixir
# Simple text generation
{:ok, text} = Gemini.text("Write a haiku about programming")

# With configuration
alias Gemini.Types.GenerationConfig

config = GenerationConfig.creative(max_output_tokens: 100)
{:ok, response} = Gemini.generate("Tell me a story", generation_config: config)
```

### Multimodal Content

```elixir
alias Gemini.Types.Content

# Text and image
contents = [
  Content.text("What's in this image?"),
  Content.image("/path/to/image.jpg")
]
{:ok, response} = Gemini.generate(contents)

# Convenience function for multiple images
prompt = Gemini.multimodal_prompt(
  "Compare these images",
  ["/path/to/image1.jpg", "/path/to/image2.png"]
)
{:ok, response} = Gemini.generate(prompt)
```

### Chat Sessions

```elixir
# Start a chat
{:ok, chat} = Gemini.chat()

# Send messages
{:ok, response, chat} = Gemini.send_message(chat, "Hello! I'm learning Elixir.")
{:ok, response, chat} = Gemini.send_message(chat, "What are the main concepts?")
```

### Real-Time Streaming

The library supports true real-time streaming where content appears progressively as it's generated:

```elixir
# Start a stream (returns immediately with stream ID)
{:ok, stream_id} = Gemini.start_stream("Write a creative story about AI")

# Subscribe to receive events as they arrive
:ok = Gemini.subscribe_stream(stream_id)

# Handle streaming events in real-time
receive do
  {:stream_event, ^stream_id, %{type: :data, data: data}} ->
    # Extract and display text as it streams in
    text = extract_text_from_stream_data(data)
    IO.write(text)
    
  {:stream_complete, ^stream_id} ->
    IO.puts("\n✅ Stream completed!")
    
  {:stream_error, ^stream_id, error} ->
    IO.puts("❌ Stream error: #{inspect(error)}")
end
```

#### Live Streaming Demo

Try the included streaming demo:

```bash
# Set up authentication (choose one):
export GEMINI_API_KEY="your_api_key_here"
# OR
export VERTEX_JSON_FILE="/path/to/service-account.json"

# Run the demo
mix run streaming_demo.exs
```

The demo shows real-time text generation where each word appears as it's generated by the AI, not all at once.

### Model Information

```elixir
# List available models
{:ok, models_response} = Gemini.list_models()
model_names = Enum.map(models_response.models, & &1.name)

# Get specific model info
{:ok, model} = Gemini.get_model("gemini-2.0-flash")
IO.inspect(model.input_token_limit)

# Check if model exists
{:ok, exists} = Gemini.model_exists?("gemini-2.0-flash")
```

### Safety and Configuration

```elixir
alias Gemini.Types.{GenerationConfig, SafetySetting}

# Configure generation parameters
config = GenerationConfig.new(
  temperature: 0.7,
  max_output_tokens: 1000,
  top_p: 0.9
)

# Set safety settings
safety_settings = SafetySetting.permissive()

{:ok, response} = Gemini.generate(
  "Write about historical events",
  generation_config: config,
  safety_settings: safety_settings
)
```

### Token Counting

```elixir
# Count tokens for cost estimation
{:ok, count} = Gemini.count_tokens("This is a test message")
IO.puts("Tokens: #{count.total_tokens}")

# With multimodal content
contents = [Content.text("Describe"), Content.image("image.jpg")]
{:ok, count} = Gemini.count_tokens(contents)
```

## Configuration Options

### Authentication Strategies

The library supports two authentication strategies that can be configured at runtime:

#### Gemini API Authentication

```elixir
# Runtime configuration (recommended)
Gemini.configure(:gemini, %{api_key: "your_api_key"})

# Or application config
config :gemini, api_key: "your_api_key"

# Or environment variable
export GEMINI_API_KEY="your_api_key"
```

#### Vertex AI Authentication

```elixir
# With access token
Gemini.configure(:vertex_ai, %{
  access_token: "your_access_token",
  project_id: "your-project-id",
  location: "us-central1"
})

# With service account JSON file
Gemini.configure(:vertex_ai, %{
  service_account_key: "/path/to/service-account.json",
  project_id: "your-project-id",  # Optional, auto-detected from JSON
  location: "us-central1"         # Optional, defaults to us-central1
})
```

### Current Configuration Model

**Important:** The current version uses a **single global configuration** that applies to all API calls. Only one authentication strategy can be active at a time.

```elixir
# This overwrites any previous configuration
Gemini.configure(:gemini, %{api_key: "gemini_key"})
{:ok, response1} = Gemini.generate("Hello")  # Uses Gemini API

# This overwrites the Gemini configuration  
Gemini.configure(:vertex_ai, %{
  service_account_key: "/path/to/key.json",
  project_id: "project"
})
{:ok, response2} = Gemini.generate("Hello")  # Uses Vertex AI

# Check current configuration
auth_config = Gemini.get_auth_config()
```

### Configuration Priority

Configuration is resolved in this order (highest to lowest priority):

1. **Environment Variables** (always takes precedence)
   - `GEMINI_API_KEY`
   - `VERTEX_JSON_FILE` / `VERTEX_SERVICE_ACCOUNT` 
   - `VERTEX_PROJECT_ID` / `GOOGLE_CLOUD_PROJECT`
   - `VERTEX_LOCATION` / `GOOGLE_CLOUD_LOCATION`

2. **Runtime Configuration** (via `Gemini.configure/2`)

3. **Application Configuration** (in `config/config.exs`)

### Multiple Adapters (Future Enhancement)

The current architecture doesn't support multiple simultaneous configurations. If you need to use both Gemini API and Vertex AI in the same application, you would need to:

1. **Switch configurations dynamically:**
   ```elixir
   # Switch to Gemini
   Gemini.configure(:gemini, %{api_key: "key"})
   {:ok, gemini_response} = Gemini.generate("Hello from Gemini")
   
   # Switch to Vertex AI
   Gemini.configure(:vertex_ai, %{service_account_key: "/path/to/key.json", project_id: "project"})
   {:ok, vertex_response} = Gemini.generate("Hello from Vertex")
   ```

2. **Use environment variables to control which auth method is active**

For true multi-adapter support, future versions could implement:
- Named adapter instances: `Gemini.Adapter.new(:gemini_prod, config)`
- Per-request configuration: `Gemini.generate(text, auth: config)`
- Process-specific configuration storage

### Additional Configuration

Configure other options in your `config/config.exs`:

```elixir
config :gemini,
  default_model: "gemini-2.0-flash",
  timeout: 30_000,
  telemetry_enabled: true,
  streaming: [
    max_concurrent_streams: 100,
    default_timeout: 30_000,
    max_retries: 3
  ]
```

### Environment Variables

- `GEMINI_API_KEY` - Your Gemini API key (overrides all other config)
- `VERTEX_JSON_FILE` / `VERTEX_SERVICE_ACCOUNT` - Path to service account JSON (overrides all other config)
- `VERTEX_PROJECT_ID` / `GOOGLE_CLOUD_PROJECT` - Google Cloud project ID (optional, auto-detected from JSON)
- `VERTEX_LOCATION` / `GOOGLE_CLOUD_LOCATION` - Google Cloud location (optional, defaults to us-central1)

## Error Handling

The library provides structured error handling:

```elixir
case Gemini.text("Hello") do
  {:ok, text} ->
    IO.puts("Success: #{text}")
  
  {:error, %Gemini.Error{type: :api_error, message: message}} ->
    IO.puts("API Error: #{message}")
  
  {:error, %Gemini.Error{type: :network_error, message: message}} ->
    IO.puts("Network Error: #{message}")
  
  {:error, %Gemini.Error{type: :config_error, message: message}} ->
    IO.puts("Config Error: #{message}")
end
```

Error types include:
- `:api_error` - Errors from the Gemini API
- `:network_error` - Connection or HTTP errors  
- `:config_error` - Configuration issues
- `:validation_error` - Request validation errors
- `:invalid_response` - Response parsing errors

## API Reference

### Core Functions

- `Gemini.generate/2` - Generate content with full options
- `Gemini.text/2` - Simple text generation
- `Gemini.count_tokens/2` - Count tokens in content

### Streaming Functions

- `Gemini.start_stream/2` - Start a real-time streaming session
- `Gemini.subscribe_stream/2` - Subscribe to stream events
- `Gemini.get_stream_status/1` - Get stream information
- `Gemini.stop_stream/1` - Stop an active stream

### Chat Functions

- `Gemini.chat/1` - Start a chat session
- `Gemini.send_message/2` - Send message in chat

### Model Functions

- `Gemini.list_models/1` - List available models
- `Gemini.get_model/1` - Get model information
- `Gemini.model_exists?/1` - Check if model exists

### Utility Functions

- `Gemini.extract_text/1` - Extract text from response
- `Gemini.multimodal_prompt/2` - Create multimodal prompts

## Type System

The library uses TypedStruct for compile-time type checking:

```elixir
alias Gemini.Types.{Content, Part, GenerationConfig, SafetySetting}

# All types are fully specified
content = %Content{
  role: "user",
  parts: [%Part{text: "Hello"}]
}
```

## Testing

### Running Unit Tests

Run the standard test suite with mocked API responses:

```bash
mix test
```

This runs all tests except live API tests, using mock adapters for fast, reliable testing.

### Running Live API Tests

To test against the actual Google APIs, you can run live integration tests. These require valid credentials and will make real API calls.

#### Gemini API Live Tests

Set up your Gemini API key and run live tests:

```bash
export GEMINI_API_KEY="your_api_key_here"
mix test --include live_api
```

#### Vertex AI Live Tests

Set up your Google Cloud service account and run live tests:

```bash
export VERTEX_JSON_FILE="/path/to/your/service-account.json"
export VERTEX_PROJECT_ID="your-gcp-project-id"  # Optional, auto-detected from JSON
export VERTEX_LOCATION="us-central1"            # Optional, defaults to us-central1
mix test --include live_api
```

#### Running Both Authentication Methods

To test both Gemini API and Vertex AI authentication:

```bash
export GEMINI_API_KEY="your_api_key_here"
export VERTEX_JSON_FILE="/path/to/your/service-account.json"
mix test --include live_api
```

#### Running Specific Live API Tests

Run only the live API test file:

```bash
# With environment variables set
mix test test/live_api_test.exs --include live_api
```

**Note:** Live API tests make real requests to Google's services and may incur costs. Use test credentials when possible.

## Examples

See [EXAMPLES.md](EXAMPLES.md) for comprehensive usage examples.

## Documentation

Generate documentation:

```bash
mix docs
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Built for the Google Gemini API
- Uses Finch for HTTP client functionality
- TypedStruct for type safety
- Jason for JSON handling
